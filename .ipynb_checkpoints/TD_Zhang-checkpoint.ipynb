{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib\n",
    "from tsfresh.examples.har_dataset import download_har_dataset, load_har_dataset, load_har_classes\n",
    "# import seaborn as sns\n",
    "from localtsfresh.tsfresh import extract_features, extract_relevant_features, select_features\n",
    "from localtsfresh.tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, EfficientFCParameters, MinimalFCParameters\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "\n",
    "import logging\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.signal import butter, lfilter, lfilter_zi\n",
    "from numpy import linalg as LA\n",
    "from xgboost import XGBClassifier\n",
    "import copy\n",
    "from collections import Counter\n",
    "\n",
    "# We set the logger to Error level\n",
    "# This is not recommend for normal use as you can oversee important Warning messages\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "#根据Win，生成DataFrame格式中的Time列\n",
    "def getTimeColumn(win):\n",
    "    time=np.arange(win)\n",
    "    for idx in range(1, sensornum):\n",
    "        timetmp=np.arange(win)\n",
    "        time=np.concatenate((time, timetmp), axis=0)\n",
    "    time.shape=(len(time),1)\n",
    "    return time\n",
    "\n",
    "#根据Win和数据，生成DataFrame格式中的kind和value列\n",
    "def getKindValueColumn(sd, win):\n",
    "    for i in range(sensornum):\n",
    "        kind=i*np.ones((win, 1),dtype=int)\n",
    "        sensorcols=sd[i]\n",
    "        sensorcols.shape=(win, 1)\n",
    "        sdata=np.column_stack((kind, sensorcols))\n",
    "        if i==0:\n",
    "            sensorframe=sdata\n",
    "        else:\n",
    "            sensorframe=np.row_stack((sensorframe, sdata))\n",
    "    return sensorframe\n",
    "\n",
    "def getIdColumn(num, win):\n",
    "    for i in range(num):\n",
    "        if i==0:\n",
    "            idary=0*np.ones((sensornum*win, 1))\n",
    "        else:\n",
    "            idary=np.row_stack((idary, i*np.ones((sensornum*win, 1))))\n",
    "    return idary\n",
    "\n",
    "def getDataLabelColumn(dataary,label, num, win):\n",
    "    for i in range(filenum):\n",
    "        if i==0:\n",
    "            y=label[travel[i]][:num]\n",
    "            data=dataary[travel[i]][:num*win*sensornum]\n",
    "        else:\n",
    "            y=np.row_stack((y, label[travel[i]][:num]))\n",
    "            data=np.row_stack((data, dataary[travel[i]][:num*win*sensornum]))\n",
    "    y.shape=(len(y),)\n",
    "    y=pd.Series(y)\n",
    "    return data, y\n",
    "\n",
    "\n",
    "#读取所有数据，结果是字典，分别存储六种运动的传感器读数矩阵，每个矩阵的三列分别是time, kind, value\n",
    "def loadData(win, step, fs, sampling):\n",
    "    labeldict={}\n",
    "    datadict={}\n",
    "    files = os.listdir('/home/hadoop/data')\n",
    "    if step<=win:\n",
    "        for i in range(0, filenum):\n",
    "            with open('/home/hadoop/data/%s' % files[i], 'r') as f:\n",
    "                sensordata=np.loadtxt(f, delimiter=\",\")[::2, :]\n",
    "                \n",
    "                ########## feature extraction ###########\n",
    "                sensordata = np.delete(sensordata, np.s_[:3], 1)\n",
    "                sensordata[:, 2] = sensordata[:, 2] - 9.8\n",
    "                sensordata = resample(sensordata, sampling)\n",
    "                \n",
    "                #mi = LA.norm(sensordata, 2, axis=1)\n",
    "                #sma = np.sum(np.abs(sensordata), 1)\n",
    "                #velo = calcVelo(sensordata[:,:2], fs, win)\n",
    "                #sensordata = np.c_[sensordata, mi, sma]\n",
    "                \n",
    "                #loadDataPlot(sensordata, i)\n",
    "                \n",
    "                ########## filter ############\n",
    "                #fltr2(sensordata, cutOff=15, fs=fs, order=5)\n",
    "                #########################################\n",
    "                \n",
    "                max_num=int((len(sensordata)-win)/step)+1\n",
    "                for j in range(0, max_num):\n",
    "                    start_idx=step*j\n",
    "                    end_idx=step*j+win\n",
    "                    sd=sensordata[start_idx:end_idx,:]\n",
    "                    sd=sd.transpose()\n",
    "                        \n",
    "                    time=getTimeColumn(win)\n",
    "                    kindvalue=getKindValueColumn(sd, win)\n",
    "                    travary=np.column_stack((time, kindvalue ))\n",
    "\n",
    "                    if j == 0:\n",
    "                        dataarray=travary\n",
    "                    else:\n",
    "                        dataarray=np.concatenate((dataarray, travary), axis=0)  \n",
    "\n",
    "                labeldict[travel[i]]=i*np.ones((max_num, 1), dtype=int)\n",
    "                datadict[travel[i]]=dataarray\n",
    "                print(files[i]+\" loaded!\")\n",
    "        use_num_max=filenum*max_num\n",
    "        return datadict, labeldict, use_num_max\n",
    "    else:\n",
    "        raise IOError('\\'step\\' of slide window shoud be less than \\'win\\'')\n",
    "\n",
    "#从所有数据dataary和其标签label中，选出总数为num的样本，每个类别选num/sensornum个样本。\n",
    "def genTrainSample(dataary, label, num, win):\n",
    "    idary=getIdColumn(num, win)\n",
    "    labelnum=int(num/filenum) ;\n",
    "    data, y=getDataLabelColumn(dataary, label, labelnum, win)\n",
    "    data=np.column_stack((idary, data))\n",
    "    dataframe = DataFrame(data, columns=['id', 'time', 'kind', 'value'])\n",
    "    return dataframe, y\n",
    "\n",
    "\n",
    "#画出第idx个样本3个传感器的数据\n",
    "def plotSample(data, kind, idx, win, use_num):\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    tmp=data.iloc[:, -1].values\n",
    "    for i in range(sensornum):\n",
    "        plt.subplot(3,2,i+1)\n",
    "        plt.title(sensor[i] + ' readings')\n",
    "        begin=kind*win*use_num + idx*win*sensornum + win*i\n",
    "        end=begin+win-1\n",
    "        plt.plot(tmp[begin:end])\n",
    "    plt.show()\n",
    "    \n",
    "def loadDataPlot(data, ind):\n",
    "    plt.figure(ind)\n",
    "    for i in xrange(0, data.shape[1]):\n",
    "        fig = matplotlib.pyplot.gcf()\n",
    "        fig.set_size_inches(18.5, 10.5)\n",
    "        plt.subplot(3,2,i+1)\n",
    "        plt.plot(data[:, i])\n",
    "    return\n",
    "\n",
    "################ Resampling ##################\n",
    "\n",
    "def resample(data, sampling):\n",
    "    tmp = []\n",
    "    for i in xrange(0, 1):\n",
    "        ind = np.arange(i, data.shape[0], sampling)\n",
    "        tmp.append(data[ind, :])\n",
    "    return np.concatenate(tmp, axis=0)\n",
    "\n",
    "################ Calculate Velocity ###################\n",
    "\n",
    "def calcVelo(acc, fs, win):\n",
    "    velo = np.zeros(acc.shape)\n",
    "    for i in xrange(0, acc.shape[0]):\n",
    "        if i % win: \n",
    "            velo[i] = velo[i-1] + (acc[i-1]+acc[i])/(2*fs)\n",
    "    return LA.norm(velo, 2, axis=1)\n",
    "        \n",
    "\n",
    "################ Butterworth 滤波 ###################\n",
    "\n",
    "def butter_lowpass(cutOff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normalCutoff = cutOff / nyq\n",
    "    b, a = butter(order, normalCutoff, btype='low', analog = False)\n",
    "    return b, a\n",
    "\n",
    "def fltr(data, win, sensornum, cutOff, fs, order=5): #这个是在滑窗之后滤波的函数\n",
    "    v = data.iloc[:, -1].values\n",
    "    b, a = butter_lowpass(cutOff, fs, order)\n",
    "    zi = np.tile(lfilter_zi(b, a), (sensornum,1))\n",
    "    for i in xrange(0, v.size, win*sensornum):\n",
    "        for j in xrange(0, sensornum):\n",
    "            x = v[i+j*win: i+(j+1)*win]\n",
    "            y, zi[j]= lfilter(b, a, x, zi=zi[j])\n",
    "            data.iloc[i+j*win: i+(j+1)*win, -1] = y\n",
    "    return\n",
    "\n",
    "def fltr2(sensordata, cutOff, fs, order):  #这个是在数据读取后马上就滤波的函数\n",
    "    b, a = butter_lowpass(cutOff, fs, order)\n",
    "    for n in xrange(sensordata.shape[1]):\n",
    "        zi = lfilter_zi(b, a)\n",
    "        sensordata[:,n], zi = lfilter(b, a, sensordata[:,n], zi=zi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bus_data_set_101 loaded!\n",
      "car_data_set_103 loaded!\n",
      "running_data_set_8 loaded!\n",
      "stationary_data_set_3 loaded!\n",
      "subway_data_set_102 loaded!\n",
      "walking_data_set_7 loaded!\n",
      "\n",
      " \u001b[1;33;48m NOTE: use_num should be less than 6072\n"
     ]
    }
   ],
   "source": [
    "#可修改常量\n",
    "# 六种出行方式\n",
    "travel=[\"bus\", \"car\", \"running\", \"stationary\", \"subway\", \"walking\"]\n",
    "# 六个传感器数据\n",
    "#sensor=[\"azimath\", \"pitch\", \"roll\", \"mi\", \"sma\"]\n",
    "sensor=[\"azimath\", \"pitch\", \"roll\"]\n",
    "# 一些常量\n",
    "filenum=6\n",
    "#窗口步长大小\n",
    "win=120\n",
    "step=60            #步长应该小于等于win\n",
    "sampling=1         #采样间隔，注意，win是在采样间隔基础上算的\n",
    "#使用的样本数量(一个窗口的数据是一个样本)，建议设置成sensornum的倍数\n",
    "fs = 100/sampling\n",
    "\n",
    "sensornum=len(sensor)\n",
    "data, label, use_num_max = loadData(win, step, fs, sampling)\n",
    "\n",
    "print(\"\\n \\033[1;33;48m NOTE: use_num should be less than %s\"%(use_num_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#取出use_num个样本，并将data转化成tsfresh需要的pandas.DataFrame类型，存储为df；将label转化为特征过滤需要的pandas.Series类型，存储为y\n",
    "# print(data[\"bus\"].shape)        #data[\"bus]矩阵三列分别是time, kind, value\n",
    "# print(label[\"bus\"].shape)       #只有一列，因为sensornum*win条读数才是一个样本，所以label的行数是data的1/(sensornum*win)\n",
    "use_num = 6072\n",
    "df, y=genTrainSample(data, label, use_num, win)\n",
    "\n",
    "######## Low Pass Filter ##########\n",
    "kind=0\n",
    "idx=10\n",
    "#plotSample(df, kind,idx, win, use_num)\n",
    "\n",
    "#fltr(df, win, sensornum, cutOff=4, fs=fs, order=5) #filter\n",
    "\n",
    "master_df = df\n",
    "print(master_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extraction_settings = ComprehensiveFCParameters()\n",
    "#extraction_settings = EfficientFCParameters()\n",
    "# extraction_settings = MinimalFCParameters()\n",
    "#extraction_settings.update({\"fft_dc\":None,\"fft_mean\":None,\"fft_var\":None,\"fft_std\":None,\"fft_kurt\":None,\"fft_shape_mean\":None,\"fft_shape_std\":None,\"fft_shape_skew\":None,\"fft_shape_kurt\":None})\n",
    "\n",
    "extraction_settings = {'ar_coefficient': [{'coeff': 0, 'k': 10},\n",
    "    {'coeff': 1, 'k': 10},\n",
    "    {'coeff': 2, 'k': 10},\n",
    "    {'coeff': 3, 'k': 10},\n",
    "    {'coeff': 4, 'k': 10}],\n",
    "          'longest_strike_above_mean': None,\n",
    "          'longest_strike_below_mean': None,\n",
    "         'mean_abs_change_quantiles': [{'qh': 0.2, 'ql': 0.0},\n",
    "                                        {'qh': 0.4, 'ql': 0.0},\n",
    "                                        {'qh': 0.6, 'ql': 0.0},\n",
    "                                        {'qh': 0.8, 'ql': 0.0},\n",
    "                                        {'qh': 1.0, 'ql': 0.0},\n",
    "                                        {'qh': 0.4, 'ql': 0.2},\n",
    "                                        {'qh': 0.6, 'ql': 0.2},\n",
    "                                        {'qh': 0.8, 'ql': 0.2},\n",
    "                                        {'qh': 1.0, 'ql': 0.2},\n",
    "                                        {'qh': 0.6, 'ql': 0.4},\n",
    "                                        {'qh': 0.8, 'ql': 0.4},\n",
    "                                        {'qh': 1.0, 'ql': 0.4},\n",
    "                                        {'qh': 0.8, 'ql': 0.6},\n",
    "                                        {'qh': 1.0, 'ql': 0.6},\n",
    "                                        {'qh': 1.0, 'ql': 0.8}],\n",
    "          'autocorrelation': [{'lag': 0},{'lag': 1},{'lag': 2},{'lag': 3},{'lag': 4},\n",
    "                              {'lag': 5},{'lag': 6},{'lag': 7},{'lag': 8},{'lag': 9}],\n",
    "          'time_reversal_asymmetry_statistic': [{'lag': 1}, {'lag': 2}, {'lag': 3}],\n",
    "          'quantile': [{'q': 0.1},{'q': 0.2},{'q': 0.3},{'q': 0.4},\n",
    "                       {'q': 0.6},{'q': 0.7},{'q': 0.8},{'q': 0.9}],\n",
    "          'number_peaks': [{'n': 1}, {'n': 3}, {'n': 5}],\n",
    "          'minimum': None,\n",
    "          'maximum': None,\n",
    "          'median': None,\n",
    "          'sum_values': None,\n",
    "          'spkt_welch_density': [{'coeff': 2}, {'coeff': 5}, {'coeff': 8}],\n",
    "                    \"fft_dc\":None,\n",
    "                    \"fft_mean\":None,\n",
    "                    #\"fft_var\":None,\n",
    "                    \"fft_std\":None,\n",
    "                    \"fft_kurt\":None,\n",
    "                    \"fft_shape_mean\":None,\n",
    "                    \"fft_shape_std\":None,\n",
    "                    \"fft_shape_skew\":None,\n",
    "                    \"fft_shape_kurt\":None\n",
    "                      }\n",
    "\n",
    "%time X = extract_features(master_df, default_fc_parameters=extraction_settings, column_id='id', column_sort=\"time\", column_kind=\"kind\", column_value=\"value\");\n",
    "# %time X = extract_features(master_df, column_id='id', column_sort=\"time\", column_kind=\"kind\", column_value=\"value\");\n",
    "\n",
    "# in total we have transformed the sensor data into 222 features\n",
    "impute(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in X.columns:\n",
    "    std = np.std(X[n])\n",
    "    X[n] = (X[n]-np.mean(X[n]))/std if std>0 else X[n]-np.mean(X[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_tmp, X_test, y_train_tmp, y_test = train_test_split(X, y, test_size=.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_tmp, y_train_tmp, test_size=.125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 整体模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.900826446281\n"
     ]
    }
   ],
   "source": [
    "#Using all the features as contrast\n",
    "eval_set = [(X_val, y_val)]\n",
    "cl = XGBClassifier(max_depth=6, n_estimators=1000, objective='multi:softmax', reg_lambda=2)\n",
    "cl.fit(X_train, y_train, eval_metric='mlogloss', eval_set=eval_set, early_stopping_rounds=30, verbose=False)\n",
    "score = accuracy_score(y_test, cl.predict(X_test))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop 0, accuracy=0.904959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "          \r",
      "Feature Extraction:  40%|████      | 1449/3618 [00:50<01:14, 28.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop 1, accuracy=0.888430\n",
      "Loop 2, accuracy=0.892562\n",
      "Loop 3, accuracy=0.917355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process PoolWorker-42:\n",
      "Process PoolWorker-41:\n",
      "Process PoolWorker-37:\n",
      "Process PoolWorker-43:\n",
      "Process PoolWorker-40:\n",
      "Process PoolWorker-44:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process PoolWorker-38:\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "Process PoolWorker-39:\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "    self.run()\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "    self.run()\n",
      "    task = get()\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self.run()\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "    task = get()\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "    self.run()\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "    self.run()\n",
      "    task = get()\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    racquire()\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 378, in get\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    task = get()\n",
      "    return recv()\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "KeyboardInterrupt\n",
      "    racquire()\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "KeyboardInterrupt\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "    task = get()\n",
      "    task = get()\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 378, in get\n",
      "    racquire()\n",
      "    return recv()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "KeyboardInterrupt\n",
      "    task = get()\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "    task = get()\n",
      "  File \"/home/hadoop/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 378, in get\n",
      "    return recv()\n",
      "KeyboardInterrupt\n",
      "    racquire()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-d2b73bea525f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0meval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'multi:softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_lambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mcl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mlogloss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loop %d, accuracy=%f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hadoop/anaconda2/lib/python2.7/site-packages/xgboost/sklearn.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose)\u001b[0m\n\u001b[1;32m    443\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                               verbose_eval=verbose)\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hadoop/anaconda2/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hadoop/anaconda2/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hadoop/anaconda2/lib/python2.7/site-packages/xgboost/core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mean = 0\n",
    "for i in xrange(0, 50):\n",
    "    X_train_tmp, X_test, y_train_tmp, y_test = train_test_split(X, y, test_size=.2)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_tmp, y_train_tmp, test_size=.125)\n",
    "    eval_set = [(X_val, y_val)]\n",
    "    cl = XGBClassifier(max_depth=6, n_estimators=1000, objective='multi:softmax', reg_lambda=2)\n",
    "    cl.fit(X_train, y_train, eval_metric='mlogloss', eval_set=eval_set, early_stopping_rounds=30, verbose=False)\n",
    "    score = accuracy_score(y_test, cl.predict(X_test))\n",
    "    print('Loop %d, accuracy=%f' % (i, score))\n",
    "    mean += score\n",
    "print('Mean accuracy is %f' % (mean/50))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0.0__ar_coefficient__k_10__coeff_0',\n",
       "       '0.0__ar_coefficient__k_10__coeff_1',\n",
       "       '0.0__ar_coefficient__k_10__coeff_2',\n",
       "       '0.0__ar_coefficient__k_10__coeff_3',\n",
       "       '0.0__ar_coefficient__k_10__coeff_4', '0.0__autocorrelation__lag_2',\n",
       "       '0.0__autocorrelation__lag_3', '0.0__autocorrelation__lag_4',\n",
       "       '0.0__autocorrelation__lag_5', '0.0__autocorrelation__lag_7',\n",
       "       '0.0__fft_dc', '0.0__fft_kurt', '0.0__fft_shape_mean',\n",
       "       '0.0__fft_shape_skew', '0.0__longest_strike_above_mean',\n",
       "       '0.0__maximum', '0.0__median', '0.0__minimum',\n",
       "       '0.0__number_peaks__n_1', '0.0__quantile__q_0.1',\n",
       "       '0.0__quantile__q_0.6', '0.0__quantile__q_0.7',\n",
       "       '0.0__quantile__q_0.8', '0.0__quantile__q_0.9',\n",
       "       '0.0__time_reversal_asymmetry_statistic__lag_1',\n",
       "       '0.0__time_reversal_asymmetry_statistic__lag_3',\n",
       "       '1.0__ar_coefficient__k_10__coeff_0',\n",
       "       '1.0__ar_coefficient__k_10__coeff_1',\n",
       "       '1.0__ar_coefficient__k_10__coeff_2',\n",
       "       '1.0__ar_coefficient__k_10__coeff_3',\n",
       "       '1.0__ar_coefficient__k_10__coeff_4', '1.0__autocorrelation__lag_3',\n",
       "       '1.0__autocorrelation__lag_6', '1.0__autocorrelation__lag_7',\n",
       "       '1.0__autocorrelation__lag_8', '1.0__autocorrelation__lag_9',\n",
       "       '1.0__fft_dc', '1.0__fft_kurt', '1.0__fft_shape_skew',\n",
       "       '1.0__fft_std', '1.0__minimum', '1.0__number_peaks__n_1',\n",
       "       '1.0__quantile__q_0.1', '1.0__quantile__q_0.4',\n",
       "       '1.0__quantile__q_0.7', '1.0__quantile__q_0.9',\n",
       "       '1.0__spkt_welch_density__coeff_2',\n",
       "       '1.0__spkt_welch_density__coeff_5', '1.0__sum_values',\n",
       "       '1.0__time_reversal_asymmetry_statistic__lag_3',\n",
       "       '2.0__ar_coefficient__k_10__coeff_0',\n",
       "       '2.0__ar_coefficient__k_10__coeff_1',\n",
       "       '2.0__ar_coefficient__k_10__coeff_2',\n",
       "       '2.0__ar_coefficient__k_10__coeff_3',\n",
       "       '2.0__ar_coefficient__k_10__coeff_4', '2.0__autocorrelation__lag_1',\n",
       "       '2.0__autocorrelation__lag_2', '2.0__autocorrelation__lag_3',\n",
       "       '2.0__autocorrelation__lag_4', '2.0__autocorrelation__lag_5',\n",
       "       '2.0__autocorrelation__lag_6', '2.0__autocorrelation__lag_7',\n",
       "       '2.0__autocorrelation__lag_8', '2.0__autocorrelation__lag_9',\n",
       "       '2.0__fft_dc', '2.0__fft_kurt', '2.0__fft_shape_kurt',\n",
       "       '2.0__fft_shape_mean', '2.0__fft_shape_skew', '2.0__fft_shape_std',\n",
       "       '2.0__fft_std', '2.0__longest_strike_above_mean', '2.0__maximum',\n",
       "       '2.0__mean_abs_change_quantiles__qh_0.6__ql_0.2',\n",
       "       '2.0__mean_abs_change_quantiles__qh_0.6__ql_0.4',\n",
       "       '2.0__mean_abs_change_quantiles__qh_0.8__ql_0.0',\n",
       "       '2.0__mean_abs_change_quantiles__qh_0.8__ql_0.2',\n",
       "       '2.0__mean_abs_change_quantiles__qh_0.8__ql_0.4',\n",
       "       '2.0__mean_abs_change_quantiles__qh_1.0__ql_0.0',\n",
       "       '2.0__mean_abs_change_quantiles__qh_1.0__ql_0.8', '2.0__median',\n",
       "       '2.0__minimum', '2.0__number_peaks__n_1', '2.0__number_peaks__n_3',\n",
       "       '2.0__number_peaks__n_5', '2.0__quantile__q_0.1',\n",
       "       '2.0__quantile__q_0.2', '2.0__quantile__q_0.3',\n",
       "       '2.0__quantile__q_0.4', '2.0__quantile__q_0.6',\n",
       "       '2.0__quantile__q_0.7', '2.0__quantile__q_0.8',\n",
       "       '2.0__quantile__q_0.9', '2.0__spkt_welch_density__coeff_5',\n",
       "       '2.0__spkt_welch_density__coeff_8', '2.0__sum_values',\n",
       "       '2.0__time_reversal_asymmetry_statistic__lag_1',\n",
       "       '2.0__time_reversal_asymmetry_statistic__lag_2',\n",
       "       '2.0__time_reversal_asymmetry_statistic__lag_3'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop 0, accuracy is 0.952263\n",
      "Loop 1, accuracy is 0.953909\n",
      "Loop 2, accuracy is 0.948148\n",
      "Loop 3, accuracy is 0.941564\n",
      "Loop 4, accuracy is 0.951440\n",
      "Loop 5, accuracy is 0.939918\n",
      "Loop 6, accuracy is 0.937449\n",
      "Loop 7, accuracy is 0.944856\n",
      "Loop 8, accuracy is 0.947325\n",
      "Loop 9, accuracy is 0.944033\n",
      "Loop 10, accuracy is 0.947325\n",
      "Loop 11, accuracy is 0.947325\n",
      "Loop 12, accuracy is 0.945679\n",
      "Loop 13, accuracy is 0.946502\n",
      "Loop 14, accuracy is 0.953909\n",
      "Loop 15, accuracy is 0.952263\n",
      "Loop 16, accuracy is 0.943210\n",
      "Loop 17, accuracy is 0.952263\n",
      "Loop 18, accuracy is 0.939095\n",
      "Loop 19, accuracy is 0.950617\n",
      "Mean Accuracy is 0.946955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:23: VisibleDeprecationWarning: non integer (and non boolean) array-likes will not be accepted as indices in the future\n"
     ]
    }
   ],
   "source": [
    "cand = 100\n",
    "loop = 20\n",
    "selected_ftr = np.array([])\n",
    "mean = 0\n",
    "for i in xrange(0, loop):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "    X_train_fs, X_val, y_train_fs, y_val = train_test_split(X_train, y_train, test_size=.25)\n",
    "    \n",
    "    eval_set = [(X_val, y_val)]\n",
    "    cl = XGBClassifier(max_depth=5, n_estimators=200, objective='multi:softmax', reg_lambda=2)\n",
    "    cl.fit(X_train, y_train, eval_metric='mlogloss', eval_set=eval_set, early_stopping_rounds=30, verbose=False)\n",
    "    score = accuracy_score(y_test, cl.predict(X_test))\n",
    "\n",
    "    mean += score\n",
    "    print('Loop %d, accuracy is %f' % (i, score))\n",
    "    selected_ftr = np.append(selected_ftr, np.argsort(-cl.feature_importances_)[:cand])\n",
    "print('Mean Accuracy is %f' % (mean/loop))\n",
    "\n",
    "ftr = []\n",
    "for key, value in dict(Counter(list(selected_ftr))).iteritems():\n",
    "    if value >= loop*1/2:\n",
    "        ftr.append(key)\n",
    "ftr2 = X.columns.values[ftr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分层模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def firstLevelY(kind, y):\n",
    "    for i in xrange(0, kind.shape[0]):\n",
    "        for j, n in enumerate(kind[i]):\n",
    "            y[np.nonzero(y==n)] = i\n",
    "    return y\n",
    "\n",
    "kind = np.array([[0, 1, 4], [2, 3, 5]])\n",
    "y_train_hier = copy.deepcopy(y_train.values)\n",
    "y_train_hier = pd.Series(firstLevelY(kind, y_train_hier))\n",
    "y_val_hier = copy.deepcopy(y_val.values)\n",
    "y_val_hier = pd.Series(firstLevelY(kind, y_val_hier))\n",
    "y_test_hier = copy.deepcopy(y_test.values)\n",
    "y_test_hier = pd.Series(firstLevelY(kind, y_test_hier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练第一层模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.984362139918\n"
     ]
    }
   ],
   "source": [
    "eval_set = [(X_val, y_val_hier)]\n",
    "cl_1st = XGBClassifier(max_depth=6, n_estimators=200, objective='binary:logistic', reg_lambda=1)\n",
    "cl_1st.fit(X_train, y_train_hier, eval_metric='error', eval_set=eval_set, early_stopping_rounds=30, verbose=False)\n",
    "score = accuracy_score(y_test_hier, cl_1st.predict(X_test))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练第二层模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.992012779553\n",
      "0.88285229202\n"
     ]
    }
   ],
   "source": [
    "X_train_human = X_train.iloc[np.nonzero(y_train_hier.values)[0],:]\n",
    "y_train_human = pd.Series(y_train.values[np.nonzero(y_train_hier.values)[0]])\n",
    "X_val_human = X_val.iloc[np.nonzero(y_val_hier.values)[0],:]\n",
    "y_val_human = pd.Series(y_val.values[np.nonzero(y_val_hier.values)[0]])\n",
    "X_test_human = X_test.iloc[np.nonzero(y_test_hier.values)[0],:]\n",
    "y_test_human = pd.Series(y_test.values[np.nonzero(y_test_hier.values)[0]])\n",
    "\n",
    "X_train_motor = X_train.iloc[np.nonzero(y_train_hier.values==0)[0],:]\n",
    "y_train_motor = pd.Series(y_train.values[np.nonzero(y_train_hier.values==0)[0]])\n",
    "X_val_motor = X_val.iloc[np.nonzero(y_val_hier.values==0)[0],:]\n",
    "y_val_motor = pd.Series(y_val.values[np.nonzero(y_val_hier.values==0)[0]])\n",
    "X_test_motor = X_test.iloc[np.nonzero(y_test_hier.values==0)[0],:]\n",
    "y_test_motor = pd.Series(y_test.values[np.nonzero(y_test_hier.values==0)[0]])\n",
    "\n",
    "############### Human Model ################\n",
    "eval_set = [(X_val_human, y_val_human)]\n",
    "cl_human = XGBClassifier(max_depth=6, n_estimators=200, objective='multi:softmax', reg_lambda=1)\n",
    "cl_human.fit(X_train_human, y_train_human, eval_metric='merror', eval_set=eval_set, early_stopping_rounds=30, verbose=False)\n",
    "score = accuracy_score(y_test_human, cl_human.predict(X_test_human))\n",
    "print(score)\n",
    "\n",
    "############## Motor Model #################\n",
    "eval_set = [(X_val_motor, y_val_motor)]\n",
    "cl_motor = XGBClassifier(max_depth=5, n_estimators=200, objective='multi:softmax', reg_lambda=1)\n",
    "cl_motor.fit(X_train_motor, y_train_motor, eval_metric='merror', eval_set=eval_set, early_stopping_rounds=30, verbose=False)\n",
    "score = accuracy_score(y_test_motor, cl_motor.predict(X_test_motor))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score = 0.928395\n"
     ]
    }
   ],
   "source": [
    "y_pred_1st = cl_1st.predict(X_test)\n",
    "correct = set(np.nonzero(y_pred_1st == y_test_hier.values)[0])\n",
    "error_num = y_test.shape[0] - len(correct)\n",
    "X_test_human = X_test.iloc[list(set(np.nonzero(y_test_hier.values)[0]) & correct), :]\n",
    "y_test_human = y_test.iloc[list(set(np.nonzero(y_test_hier.values)[0]) & correct)]\n",
    "X_test_motor = X_test.iloc[list(set(np.nonzero(y_test_hier.values==0)[0]) & correct), :]\n",
    "y_test_motor = y_test.iloc[list(set(np.nonzero(y_test_hier.values==0)[0]) & correct)]\n",
    "\n",
    "y_pred_human = cl_human.predict(X_test_human)\n",
    "error_num += np.nonzero(y_pred_human != y_test_human.values)[0].size\n",
    "y_pred_motor = cl_motor.predict(X_test_motor)\n",
    "error_num += np.nonzero(y_pred_motor != y_test_motor.values)[0].size\n",
    "print('Score = %f' % (1-float(error_num)/y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ftr=['0.0__ar_coefficient__k_10__coeff_0',\n",
    "       '0.0__longest_strike_below_mean',\n",
    "       '2.0__mean_abs_change_quantiles__qh_0.6__ql_0.2',\n",
    "       '2.0__mean_abs_change_quantiles__qh_0.6__ql_0.4',\n",
    "       '1.0__autocorrelation__lag_5', '2.0__longest_strike_above_mean',\n",
    "       '2.0__autocorrelation__lag_1',\n",
    "       '2.0__mean_abs_change_quantiles__qh_1.0__ql_0.2',\n",
    "       '2.0__time_reversal_asymmetry_statistic__lag_3',\n",
    "       '1.0__quantile__q_0.6', '1.0__quantile__q_0.2',\n",
    "       '1.0__quantile__q_0.1',\n",
    "       '2.0__mean_abs_change_quantiles__qh_0.8__ql_0.2',\n",
    "       '2.0__mean_abs_change_quantiles__qh_0.8__ql_0.4',\n",
    "       '1.0__mean_abs_change_quantiles__qh_1.0__ql_0.0',\n",
    "       '2.0__number_peaks__n_5', '2.0__number_peaks__n_3',\n",
    "       '2.0__number_peaks__n_1', '1.0__minimum',\n",
    "       '2.0__autocorrelation__lag_9', '2.0__autocorrelation__lag_3',\n",
    "       '2.0__autocorrelation__lag_7', '2.0__autocorrelation__lag_6',\n",
    "       '2.0__autocorrelation__lag_5', '1.0__longest_strike_above_mean',\n",
    "       '2.0__longest_strike_below_mean', '4.0__number_peaks__n_1',\n",
    "       '4.0__mean_abs_change_quantiles__qh_0.4__ql_0.0', '2.0__median',\n",
    "       '1.0__ar_coefficient__k_10__coeff_0',\n",
    "       '1.0__ar_coefficient__k_10__coeff_1',\n",
    "       '1.0__ar_coefficient__k_10__coeff_2', '2.0__quantile__q_0.7',\n",
    "       '2.0__quantile__q_0.6', '2.0__quantile__q_0.2',\n",
    "       '2.0__quantile__q_0.9', '2.0__quantile__q_0.8',\n",
    "       '4.0__mean_abs_change_quantiles__qh_0.8__ql_0.0', '2.0__sum_values',\n",
    "       '2.0__quantile__q_0.4',\n",
    "       '2.0__mean_abs_change_quantiles__qh_0.4__ql_0.2',\n",
    "       '4.0__mean_abs_change_quantiles__qh_0.2__ql_0.0',\n",
    "       '2.0__ar_coefficient__k_10__coeff_0',\n",
    "       '2.0__ar_coefficient__k_10__coeff_1',\n",
    "       '2.0__ar_coefficient__k_10__coeff_2',\n",
    "       '2.0__ar_coefficient__k_10__coeff_3',\n",
    "       '1.0__mean_abs_change_quantiles__qh_1.0__ql_0.6',\n",
    "       '0.0__autocorrelation__lag_2', '2.0__autocorrelation__lag_2',\n",
    "       '0.0__autocorrelation__lag_9',\n",
    "       '1.0__mean_abs_change_quantiles__qh_0.8__ql_0.2',\n",
    "       '1.0__spkt_welch_density__coeff_2',\n",
    "       '2.0__time_reversal_asymmetry_statistic__lag_1', '4.0__minimum',\n",
    "       '2.0__spkt_welch_density__coeff_5',\n",
    "       '2.0__spkt_welch_density__coeff_8', '0.0__number_peaks__n_1',\n",
    "       '0.0__mean_abs_change_quantiles__qh_1.0__ql_0.0',\n",
    "       '4.0__spkt_welch_density__coeff_2', '3.0__quantile__q_0.1',\n",
    "       '0.0__ar_coefficient__k_10__coeff_2', '0.0__quantile__q_0.1',\n",
    "       '0.0__maximum', '3.0__number_peaks__n_5', '4.0__quantile__q_0.1',\n",
    "       '0.0__autocorrelation__lag_5', '0.0__autocorrelation__lag_6',\n",
    "       '5.0__quantile__q_0.2', '0.0__ar_coefficient__k_10__coeff_1',\n",
    "       '0.0__ar_coefficient__k_10__coeff_3', '3.0__quantile__q_0.2',\n",
    "       '5.0__maximum', '3.0__number_peaks__n_1',\n",
    "       '3.0__ar_coefficient__k_10__coeff_0', '1.0__number_peaks__n_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.932098765432\n"
     ]
    }
   ],
   "source": [
    "#ftr = X.columns.values[np.argsort(-cl.feature_importances_)[:50]]\n",
    "#ftr = X.columns.values[ftr]\n",
    "#print(ftr)\n",
    "#ftr = new_ftr\n",
    "tmp_X_train = X_train.loc[:, ftr]\n",
    "tmp_X_test = X_test.loc[:, ftr]\n",
    "fs_cl = XGBClassifier()\n",
    "fs_cl.fit(tmp_X_train, y_train)\n",
    "score = accuracy_score(y_test, fs_cl.predict(tmp_X_test))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ftr2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-08ad54385c1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m       'mean_autocorrelation', 'sum_of_reoccurring_values', 'abs_energy', 'agg_linear_trend__f_agg', 'kurtosis']\n\u001b[1;32m      5\u001b[0m \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mftr2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdlt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ftr2' is not defined"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "dlt = ['range_count', 'augmented_dickey_fuller', 'binned_entropy', 'number_cwt_peaks', 'friedrich_coefficients',\n",
    "      'mean_autocorrelation', 'sum_of_reoccurring_values', 'abs_energy', 'agg_linear_trend__f_agg', 'kurtosis']\n",
    "ind = []\n",
    "for i, string in enumerate(selected_ftr):\n",
    "    for s in dlt:\n",
    "        if string.find(s) >= 0:\n",
    "            ind.append(i)\n",
    "            break\n",
    "new_ftr = np.delete(selected_ftr, ind)\n",
    "new_ftr\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:6: VisibleDeprecationWarning: non integer (and non boolean) array-likes will not be accepted as indices in the future\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop = 1, Maximum Score = 0.729767, Feature Num = 1\n",
      "Loop = 2, Maximum Score = 0.761317, Feature Num = 2\n",
      "Loop = 3, Maximum Score = 0.836077, Feature Num = 3\n",
      "Loop = 4, Maximum Score = 0.877915, Feature Num = 4\n",
      "Loop = 5, Maximum Score = 0.888889, Feature Num = 5\n",
      "Loop = 6, Maximum Score = 0.899177, Feature Num = 6\n",
      "Loop = 7, Maximum Score = 0.901920, Feature Num = 7\n",
      "Loop = 8, Maximum Score = 0.908779, Feature Num = 8\n",
      "Loop = 9, Maximum Score = 0.912894, Feature Num = 9\n",
      "Loop = 10, Maximum Score = 0.920439, Feature Num = 9\n",
      "Loop = 11, Maximum Score = 0.921811, Feature Num = 10\n",
      "Loop = 12, Maximum Score = 0.921811, Feature Num = 10\n",
      "Feature Selection Completed!\n"
     ]
    }
   ],
   "source": [
    "SFFS_ftr = SFFS(X_train_fs, y_train_fs, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ftr = SFFS_ftr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test data is 0.879973\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the selected features based on Random Forest\n",
    "tmp_X_train = X_train.loc[:, ftr]\n",
    "tmp_X_test = X_test.loc[:, ftr]\n",
    "cl = XGBClassifier()\n",
    "cl.fit(tmp_X_train, y_train)\n",
    "score = accuracy_score(y_test, cl.predict(tmp_X_test))\n",
    "print('Accuracy of test data is %f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num of missclassification is 250\n",
      "Num of missclassification from Motor is 202\n"
     ]
    }
   ],
   "source": [
    "y_prdct = cl.predict(X_test)\n",
    "ind = np.nonzero(y_test.values != y_prdct)\n",
    "error = np.c_[y_test.values[ind], y_prdct[ind]]\n",
    "print('Total num of missclassification is %d' % error.size)\n",
    "\n",
    "dlt = []\n",
    "dct = [2, 3, 5]\n",
    "for i, n in enumerate(error):\n",
    "    if n[0] in dct or n[1] in dct:\n",
    "        dlt.append(i)\n",
    "motor_error = np.delete(error, dlt, 0)\n",
    "print('Num of missclassification from Motor is %d' % motor_error.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

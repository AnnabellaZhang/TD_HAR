{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用时序划分训练测试集的尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "localtsfresh_init!\n",
      "feature_extraction_init\n",
      "tsfresh_init!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib\n",
    "from tsfresh.examples.har_dataset import download_har_dataset, load_har_dataset, load_har_classes\n",
    "# import seaborn as sns\n",
    "from localtsfresh.tsfresh import extract_features, extract_relevant_features, select_features\n",
    "from localtsfresh.tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, EfficientFCParameters, MinimalFCParameters\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "\n",
    "import logging\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.signal import butter, lfilter, lfilter_zi\n",
    "from numpy import linalg as LA\n",
    "from xgboost import XGBClassifier\n",
    "import copy\n",
    "from collections import Counter\n",
    "\n",
    "# We set the logger to Error level\n",
    "# This is not recommend for normal use as you can oversee important Warning messages\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "#根据Win，生成DataFrame格式中的Time列\n",
    "def getTimeColumn(win):\n",
    "    time=np.arange(win)\n",
    "    for idx in range(1, sensornum):\n",
    "        timetmp=np.arange(win)\n",
    "        time=np.concatenate((time, timetmp), axis=0)\n",
    "    time.shape=(len(time),1)\n",
    "    return time\n",
    "\n",
    "#根据Win和数据，生成DataFrame格式中的kind和value列\n",
    "def getKindValueColumn(sd, win):\n",
    "    for i in range(sensornum):\n",
    "        kind=i*np.ones((win, 1),dtype=int)\n",
    "        sensorcols=sd[i]\n",
    "        sensorcols.shape=(win, 1)\n",
    "        sdata=np.column_stack((kind, sensorcols))\n",
    "        if i==0:\n",
    "            sensorframe=sdata\n",
    "        else:\n",
    "            sensorframe=np.row_stack((sensorframe, sdata))\n",
    "    return sensorframe\n",
    "\n",
    "def getIdColumn(num, win):\n",
    "    for i in range(num):\n",
    "        if i==0:\n",
    "            idary=0*np.ones((sensornum*win, 1))\n",
    "        else:\n",
    "            idary=np.row_stack((idary, i*np.ones((sensornum*win, 1))))\n",
    "    return idary\n",
    "\n",
    "def getDataLabelColumn(dataary,label, num, win):\n",
    "    for i in range(filenum):\n",
    "        if i==0:\n",
    "            y=label[travel[i]][:num]\n",
    "            data=dataary[travel[i]][:num*win*sensornum]\n",
    "        else:\n",
    "            y=np.row_stack((y, label[travel[i]][:num]))\n",
    "            data=np.row_stack((data, dataary[travel[i]][:num*win*sensornum]))\n",
    "    y.shape=(len(y),)\n",
    "    y=pd.Series(y)\n",
    "    return data, y\n",
    "\n",
    "\n",
    "#读取所有数据，结果是字典，分别存储六种运动的传感器读数矩阵，每个矩阵的三列分别是time, kind, value\n",
    "def loadData(win, step, sampling, begin, end):\n",
    "    labeldict={}\n",
    "    datadict={}\n",
    "    files = os.listdir('/home/hadoop/data')\n",
    "    if step<=win:\n",
    "        for i in range(0, filenum):\n",
    "            with open('/home/hadoop/data/%s' % files[i], 'r') as f:\n",
    "                sensordata=np.loadtxt(f, delimiter=\",\")[::2, :]\n",
    "                \n",
    "                ########## feature extraction ###########\n",
    "                sensordata = np.delete(sensordata, np.s_[:3], 1)\n",
    "                sensordata[:, 2] = sensordata[:, 2] - 9.8\n",
    "                sensordata = sensordata[int(sensordata.shape[0]*begin): int(sensordata.shape[0]*end)]\n",
    "                sensordata = resample(sensordata, sampling)\n",
    "                \n",
    "                #mi = LA.norm(sensordata, 2, axis=1)\n",
    "                #sma = np.sum(np.abs(sensordata), 1)\n",
    "                #velo = calcVelo(sensordata[:,:2], fs, win)\n",
    "                #sensordata = np.c_[sensordata, mi, sma]\n",
    "                \n",
    "                #loadDataPlot(sensordata, i)\n",
    "                \n",
    "                ########## filter ############\n",
    "                #fltr2(sensordata, cutOff=15, fs=fs, order=5)\n",
    "                #########################################\n",
    "                \n",
    "                max_num=int((len(sensordata)-win)/step)+1\n",
    "                for j in range(0, max_num):\n",
    "                    start_idx=step*j\n",
    "                    end_idx=step*j+win\n",
    "                    sd=sensordata[start_idx:end_idx,:]\n",
    "                    sd=sd.transpose()\n",
    "                        \n",
    "                    time=getTimeColumn(win)\n",
    "                    kindvalue=getKindValueColumn(sd, win)\n",
    "                    travary=np.column_stack((time, kindvalue ))\n",
    "\n",
    "                    if j == 0:\n",
    "                        dataarray=travary\n",
    "                    else:\n",
    "                        dataarray=np.concatenate((dataarray, travary), axis=0)  \n",
    "\n",
    "                labeldict[travel[i]]=i*np.ones((max_num, 1), dtype=int)\n",
    "                datadict[travel[i]]=dataarray\n",
    "                print(files[i]+\" loaded!\")\n",
    "        use_num_max=filenum*max_num\n",
    "        return datadict, labeldict, use_num_max\n",
    "    else:\n",
    "        raise IOError('\\'step\\' of slide window shoud be less than \\'win\\'')\n",
    "\n",
    "#从所有数据dataary和其标签label中，选出总数为num的样本，每个类别选num/sensornum个样本。\n",
    "def genTrainSample(dataary, label, num, win):\n",
    "    idary=getIdColumn(num, win)\n",
    "    labelnum=int(num/filenum) ;\n",
    "    data, y=getDataLabelColumn(dataary, label, labelnum, win)\n",
    "    data=np.column_stack((idary, data))\n",
    "    dataframe = DataFrame(data, columns=['id', 'time', 'kind', 'value'])\n",
    "    return dataframe, y\n",
    "\n",
    "\n",
    "#画出第idx个样本3个传感器的数据\n",
    "def plotSample(data, kind, idx, win, use_num):\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    tmp=data.iloc[:, -1].values\n",
    "    for i in range(sensornum):\n",
    "        plt.subplot(3,2,i+1)\n",
    "        plt.title(sensor[i] + ' readings')\n",
    "        begin=kind*win*use_num + idx*win*sensornum + win*i\n",
    "        end=begin+win-1\n",
    "        plt.plot(tmp[begin:end])\n",
    "    plt.show()\n",
    "    \n",
    "def loadDataPlot(data, ind):\n",
    "    plt.figure(ind)\n",
    "    for i in xrange(0, data.shape[1]):\n",
    "        fig = matplotlib.pyplot.gcf()\n",
    "        fig.set_size_inches(18.5, 10.5)\n",
    "        plt.subplot(3,2,i+1)\n",
    "        plt.plot(data[:, i])\n",
    "    return\n",
    "\n",
    "################ Resampling ##################\n",
    "\n",
    "def resample(data, sampling):\n",
    "    tmp = []\n",
    "    for i in xrange(0, 1):\n",
    "        ind = np.arange(i, data.shape[0], sampling)\n",
    "        tmp.append(data[ind, :])\n",
    "    return np.concatenate(tmp, axis=0)\n",
    "\n",
    "################ Calculate Velocity ###################\n",
    "\n",
    "def calcVelo(acc, fs, win):\n",
    "    velo = np.zeros(acc.shape)\n",
    "    for i in xrange(0, acc.shape[0]):\n",
    "        if i % win: \n",
    "            velo[i] = velo[i-1] + (acc[i-1]+acc[i])/(2*fs)\n",
    "    return LA.norm(velo, 2, axis=1)\n",
    "        \n",
    "\n",
    "################ Butterworth 滤波 ###################\n",
    "\n",
    "def butter_lowpass(cutOff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normalCutoff = cutOff / nyq\n",
    "    b, a = butter(order, normalCutoff, btype='low', analog = False)\n",
    "    return b, a\n",
    "\n",
    "def fltr(data, win, sensornum, cutOff, fs, order=5): #这个是在滑窗之后滤波的函数\n",
    "    v = data.iloc[:, -1].values\n",
    "    b, a = butter_lowpass(cutOff, fs, order)\n",
    "    zi = np.tile(lfilter_zi(b, a), (sensornum,1))\n",
    "    for i in xrange(0, v.size, win*sensornum):\n",
    "        for j in xrange(0, sensornum):\n",
    "            x = v[i+j*win: i+(j+1)*win]\n",
    "            y, zi[j]= lfilter(b, a, x, zi=zi[j])\n",
    "            data.iloc[i+j*win: i+(j+1)*win, -1] = y\n",
    "    return\n",
    "\n",
    "def fltr2(sensordata, cutOff, fs, order):  #这个是在数据读取后马上就滤波的函数\n",
    "    b, a = butter_lowpass(cutOff, fs, order)\n",
    "    for n in xrange(sensordata.shape[1]):\n",
    "        zi = lfilter_zi(b, a)\n",
    "        sensordata[:,n], zi = lfilter(b, a, sensordata[:,n], zi=zi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bus_data_set_101 loaded!\n",
      "car_data_set_103 loaded!\n",
      "running_data_set_8 loaded!\n",
      "stationary_data_set_3 loaded!\n",
      "subway_data_set_102 loaded!\n",
      "walking_data_set_7 loaded!\n",
      "(1526400, 4)\n",
      "It is ok!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 7632/7632 [01:29<00:00, 85.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2544, 159)\n",
      "bus_data_set_101 loaded!\n",
      "car_data_set_103 loaded!\n",
      "running_data_set_8 loaded!\n",
      "stationary_data_set_3 loaded!\n",
      "subway_data_set_102 loaded!\n",
      "walking_data_set_7 loaded!\n",
      "(212400, 4)\n",
      "It is ok!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 1062/1062 [00:12<00:00, 83.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354, 159)\n",
      "bus_data_set_101 loaded!\n",
      "car_data_set_103 loaded!\n",
      "running_data_set_8 loaded!\n",
      "stationary_data_set_3 loaded!\n",
      "subway_data_set_102 loaded!\n",
      "walking_data_set_7 loaded!\n",
      "(432000, 4)\n",
      "It is ok!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 2160/2160 [00:26<00:00, 82.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 159)\n"
     ]
    }
   ],
   "source": [
    "#可修改常量\n",
    "# 六种出行方式\n",
    "travel=[\"bus\", \"car\", \"running\", \"stationary\", \"subway\", \"walking\"]\n",
    "# 六个传感器数据\n",
    "#sensor=[\"azimath\", \"pitch\", \"roll\", \"mi\", \"sma\"]\n",
    "sensor=[\"azimath\", \"pitch\", \"roll\"]\n",
    "# 一些常量\n",
    "filenum=6\n",
    "#窗口步长大小\n",
    "sampling=1         #采样间隔，注意，win是在采样间隔基础上算的\n",
    "win=200/sampling\n",
    "step=win/2           #步长应该小于等于win\n",
    "#使用的样本数量(一个窗口的数据是一个样本)，建议设置成sensornum的倍数\n",
    "fs = 100/sampling\n",
    "\n",
    "sensornum=len(sensor)\n",
    "\n",
    "################# 划分数据集 ########################\n",
    "\n",
    "lst_X = []\n",
    "lst_y = []\n",
    "begin = [0, 0.7, 0.8]\n",
    "end = [0.7, 0.8, 1]\n",
    "for i in xrange(0,3):\n",
    "    data, label, use_num_max = loadData(win, step, sampling, begin[i], end[i])\n",
    "    \n",
    "    use_num = use_num_max\n",
    "    df, y=genTrainSample(data, label, use_num, win)\n",
    "\n",
    "    #fltr(df, win, sensornum, cutOff=4, fs=fs, order=5) #filter\n",
    "\n",
    "    master_df = df\n",
    "    print(master_df.shape)\n",
    "    \n",
    "    # extraction_settings = ComprehensiveFCParameters()\n",
    "    # extraction_settings = EfficientFCParameters()\n",
    "    # extraction_settings = MinimalFCParameters()\n",
    "    #extraction_settings.update({\"fft_dc\":None,\"fft_mean\":None,\"fft_var\":None,\"fft_std\":None,\"fft_kurt\":None,\"fft_shape_mean\":None,\"fft_shape_std\":None,\"fft_shape_skew\":None,\"fft_shape_kurt\":None})\n",
    "    #\"\"\"\n",
    "    extraction_settings = {'ar_coefficient': [{'coeff': 0, 'k': 10},\n",
    "        {'coeff': 1, 'k': 10},\n",
    "        {'coeff': 2, 'k': 10},\n",
    "        {'coeff': 3, 'k': 10},\n",
    "        {'coeff': 4, 'k': 10}],\n",
    "              'longest_strike_above_mean': None,\n",
    "              'longest_strike_below_mean': None,\n",
    "            'mean_abs_change_quantiles': [{'qh': 0.2, 'ql': 0.0},\n",
    "                                          {'qh': 0.4, 'ql': 0.0},\n",
    "                                          {'qh': 0.6, 'ql': 0.0},\n",
    "                                          {'qh': 0.8, 'ql': 0.0},\n",
    "                                          {'qh': 1.0, 'ql': 0.0},\n",
    "                                          {'qh': 0.4, 'ql': 0.2},                                       \n",
    "                                          {'qh': 0.6, 'ql': 0.2},\n",
    "                                          {'qh': 0.8, 'ql': 0.2},\n",
    "                                          {'qh': 1.0, 'ql': 0.2},\n",
    "                                          {'qh': 0.6, 'ql': 0.4},\n",
    "                                          {'qh': 0.8, 'ql': 0.4},\n",
    "                                          {'qh': 1.0, 'ql': 0.4},\n",
    "                                          {'qh': 0.8, 'ql': 0.6},\n",
    "                                          {'qh': 1.0, 'ql': 0.6},\n",
    "                                          {'qh': 1.0, 'ql': 0.8}],\n",
    "              'autocorrelation': [{'lag': 0},{'lag': 1},{'lag': 2},{'lag': 3},{'lag': 4},\n",
    "                                  {'lag': 5},{'lag': 6},{'lag': 7},{'lag': 8},{'lag': 9}],\n",
    "              'time_reversal_asymmetry_statistic': [{'lag': 1}, {'lag': 2}, {'lag': 3}],\n",
    "              'quantile': [{'q': 0.1},{'q': 0.2},{'q': 0.3},{'q': 0.4},\n",
    "                           {'q': 0.6},{'q': 0.7},{'q': 0.8},{'q': 0.9}],\n",
    "              'number_peaks': [{'n': 1}, {'n': 3}, {'n': 5}],\n",
    "              'minimum': None,\n",
    "              'maximum': None,\n",
    "              'median': None,\n",
    "              'sum_values': None,\n",
    "              'spkt_welch_density': [{'coeff': 2}, {'coeff': 5}, {'coeff': 8}],\n",
    "    }\n",
    "    #\"\"\"\n",
    "    X = extract_features(master_df, default_fc_parameters=extraction_settings, column_id='id', \n",
    "                         column_sort=\"time\", column_kind=\"kind\", column_value=\"value\")\n",
    "\n",
    "    impute(X)\n",
    "    print(X.shape)\n",
    "    for n in X.columns:\n",
    "        std = np.std(X[n])\n",
    "        X[n] = (X[n]-np.mean(X[n]))/std if std>0 else X[n]-np.mean(X[n])\n",
    "    lst_X.append(copy.deepcopy(X))\n",
    "    lst_y.append(copy.deepcopy(y))\n",
    "\n",
    "X_train = lst_X[0]\n",
    "y_train = lst_y[0]\n",
    "X_val = lst_X[1]\n",
    "y_val = lst_y[1]\n",
    "X_test = lst_X[2]\n",
    "y_test = lst_y[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.979167, Validation Accuracy: 0.539548, Test Accuracy: 0.740278\n"
     ]
    }
   ],
   "source": [
    "#Using all the features as contrast\n",
    "eval_set = [(X_val, y_val)]\n",
    "cl = XGBClassifier(max_depth=5, n_estimators=200, objective='multi:softmax', reg_alpha=0, reg_lambda=0,\n",
    "                  subsample=0.75, gamma=5)\n",
    "cl.fit(X_train, y_train, eval_metric='merror', eval_set=eval_set, early_stopping_rounds=30, verbose=False)\n",
    "print('Train Accuracy: %f, Validation Accuracy: %f, Test Accuracy: %f' % \n",
    "      (accuracy_score(y_train, cl.predict(X_train)), accuracy_score(y_val, cl.predict(X_val)), accuracy_score(y_test, cl.predict(X_test))))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
